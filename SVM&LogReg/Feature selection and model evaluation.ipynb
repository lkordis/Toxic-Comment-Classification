{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, SelectKBest\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "subm = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['none'] = 1-train[label_cols].max(axis=1)\n",
    "train.describe()\n",
    "\n",
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('trn_term_doc.pkl', 'rb') as input:\n",
    "    trn_term_doc = pickle.load(input)\n",
    "    \n",
    "with open('test_term_doc.pkl', 'rb') as input:\n",
    "    test_term_doc = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NE Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk\n",
    "\n",
    "my_sent = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "get_continuous_chunks(my_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, lang_detect_exception\n",
    "\n",
    "detect(\"Ja sam Lovro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trn_term_doc.copy()\n",
    "X_test = test_term_doc.copy()\n",
    "\n",
    "selected_features = []\n",
    "test_selected_features = []\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    y = train[j].values\n",
    "    selector = SelectKBest(chi2,k=500000)\n",
    "    ts = selector.fit_transform(X_train,y)\n",
    "    tested = selector.transform(X_test)\n",
    "    selected_features.append(ts)\n",
    "    test_selected_features.append(tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lang_detect = []\n",
    "train_ne = []\n",
    "\n",
    "i==0\n",
    "for txt in train[COMMENT]:\n",
    "    if i % 1000 == 0 : print(i)\n",
    "    i += 1\n",
    "    try:\n",
    "        if detect(txt) != 'en':\n",
    "            train_lang_detect.append(10)\n",
    "        else:\n",
    "            train_lang_detect.append(0)\n",
    "    except:\n",
    "        train_lang_detect.append(0)\n",
    "        pass\n",
    "    \n",
    "    train_ne.append(len(get_continuous_chunks(txt)))\n",
    "    \n",
    "train_lang_detect = np.asarray(train_lang_detect)\n",
    "train_ne = np.asarray(train_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('extra_features_lang.pkl', 'wb') as output:\n",
    "    pickle.dump(train_lang_detect, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('extra_features_ne.pkl', 'wb') as output:\n",
    "    pickle.dump(train_ne, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('extra_features_lang.pkl', 'rb') as input:\n",
    "    train_lang_detect = pickle.load(input)\n",
    "\n",
    "with open('extra_features_ne.pkl', 'rb') as input:\n",
    "    train_ne = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "lang_dec_T = np.array([train_lang_detect]).T\n",
    "ne_T = np.array([train_ne]).T\n",
    "\n",
    "for i in range(len(selected_features)):\n",
    "    s = selected_features[i]\n",
    "    s = hstack((s,ne_T))\n",
    "    s = hstack((s,lang_dec_T))\n",
    "    selected_features[i] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nested_kfold_cv(clf, param_grid, X, y, k1=3, k2=3):\n",
    "    \n",
    "    acc, precision, recall, f1 = [],[],[],[]\n",
    "    kfold = KFold(n_splits=k1, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Outer loop\n",
    "    for ind_train, ind_test in kfold.split(X):\n",
    "        \n",
    "        X_train, y_train, X_test, y_test = X[ind_train], y[ind_train], X[ind_test], y[ind_test]\n",
    "        \n",
    "        # Inner loop\n",
    "        inn = GridSearchCV(clf, param_grid, cv=k2, n_jobs = -1).fit(X_train, y_train)\n",
    "        \n",
    "        # Prediction based on the best selected params, the ones that minimize average error\n",
    "        h = inn.best_estimator_.fit(X_train, y_train).predict(X_test)\n",
    "        \n",
    "        acc.append(accuracy_score(y_test, h))\n",
    "        precision.append(precision_score(y_test, h))\n",
    "        recall.append(recall_score(y_test, h))\n",
    "        f1.append(f1_score(y_test, h))\n",
    "        \n",
    "    return np.mean(acc), np.mean(precision), np.mean(recall), np.mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [2**i for i in range(-5,6)]\n",
    "param = [{'C': Cs}]\n",
    "\n",
    "avg_acc, avg_precision, avg_recall, avg_f1 = [],[],[],[]\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    acc, precision, recall, f1 = nested_kfold_cv(LogisticRegression(class_weight=\"balanced\"),param, selected_features[i].tocsc(), train[j].values)\n",
    "    avg_acc.append(acc)\n",
    "    avg_precision.append(precision)\n",
    "    avg_recall.append(recall)\n",
    "    avg_f1.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(avg_acc)\n",
    "precision =  np.mean(avg_precision)\n",
    "recall = np.mean(avg_recall)\n",
    "f1 = np.mean(avg_f1)\n",
    "\n",
    "print(acc,precision,recall,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs = [2**i for i in range(-5,6)]\n",
    "param = [{'C': Cs}]\n",
    "\n",
    "avg_acc, avg_precision, avg_recall, avg_f1 = [],[],[],[]\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    acc, precision, recall, f1 = nested_kfold_cv(LinearSVC(class_weight=\"balanced\"),param, selected_features[i].tocsc(), train[j].values)\n",
    "    avg_acc.append(acc)\n",
    "    avg_precision.append(precision)\n",
    "    avg_recall.append(recall)\n",
    "    avg_f1.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = np.mean(avg_acc)\n",
    "precision =  np.mean(avg_precision)\n",
    "recall = np.mean(avg_recall)\n",
    "f1 = np.mean(avg_f1)\n",
    "\n",
    "print(acc,precision,recall,f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
